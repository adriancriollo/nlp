{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 4\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names: __Adrian Criollo__ (Write these in every notebook you submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Neural Networks (20 points)\n",
    "----\n",
    "\n",
    "Next, we'll train a feedforward neural net to work with this data. You'll train one neural net which takes the same input as your Logistic Regression model - a sparse vector representing documents as bags of words.\n",
    "\n",
    "Take a look at these videos to understand forward and backward propagation in neural networks - \n",
    "* https://www.youtube.com/watch?v=HHbjpDHcJVw\n",
    "* https://youtu.be/-Lavz_I4l2U?si=zi20DB3qKPLMEPt1\n",
    "  \n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Neural Networks) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "import sentiment_utils as sutils\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# you can experiment with having some Dropout layers if you'd like to\n",
    "# this is not required\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# if you want to use this again\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_docs, train_labels = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_docs, dev_labels = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "# you may use either your sparse vectors or sklearn's CountVectorizer's sparse vectors\n",
    "# you will experiment with multinomial and binarized representations later\n",
    "\n",
    "# Join tokenized words into a single string for each document (for CountVectorizer)\n",
    "train_docs_joined = [' '.join(doc) for doc in train_docs]\n",
    "dev_docs_joined = [' '.join(doc) for doc in dev_docs]\n",
    "\n",
    "# Vectorize the data using CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=False)  # Set binary=True for binarized representation later\n",
    "X_train = vectorizer.fit_transform(train_docs_joined).toarray()\n",
    "X_dev = vectorizer.transform(dev_docs_joined).toarray()\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_dev = np.array(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feedforward neural network model\n",
    "# that takes a sparse BoW representation of the data as input\n",
    "# and makes a binary classification of positive/negative sentiment as output\n",
    "# you may use any number of hidden layers >= 1 and any number of units in each hidden layer (we recommend between 50-200)\n",
    "# you may use any activation function on the hidden layers \n",
    "# you should use a sigmoid activation function on the output layer\n",
    "# you should use binary cross-entropy as your loss function\n",
    "# sgd is an appropriate optimizer for this task\n",
    "# you should report accuracy as your metric\n",
    "# you may add Dropout layers if you'd like to\n",
    "\n",
    "# create/compile your model in this cell\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "# call compile here\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many trainable parameters does your model have? __2,269,901__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model\n",
    "# reports an accuracy of 0.78 at that point using the sgd optimizer\n",
    "# Ensure labels are converted to NumPy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_dev = np.array(dev_labels)\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_dev, y_dev),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# After training, evaluate the model\n",
    "loss, accuracy = model.evaluate(X_dev, y_dev, verbose=0)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "# Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\n",
    "# indicates you should change a list into a numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction on the dev set\n",
    "# then make a classification decision based on that prediction\n",
    "\n",
    "predictions = model.predict(X_dev)\n",
    "predicted_labels = [1 if prob > 0.5 else 0 for prob in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use the model.evaluate function to report the loss and accuracy on the dev set\n",
    "loss, accuracy = model.evaluate(X_dev, y_dev, verbose=1)\n",
    "print(f\"Dev Set Loss: {loss}\")\n",
    "print(f\"Dev Set Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the same graph as with NB and LR, with your neural network model instead!\n",
    "# make sure to re-create your model each time you train it â€” you don't want to start with\n",
    "# an already trained network!\n",
    "\n",
    "# you should experiment with different numbers of epochs to see how performance varies\n",
    "# you need not create an experiment that takes > 10 min to run (gradescope will run out of computing resources and give you a 0)\n",
    "\n",
    "def prepare_features(train_docs, dev_docs, train_labels, dev_labels, binary=False):\n",
    "    train_docs_joined = [' '.join(doc) for doc in train_docs]\n",
    "    dev_docs_joined = [' '.join(doc) for doc in dev_docs]\n",
    "    vectorizer = CountVectorizer(binary=binary)\n",
    "    X_train = vectorizer.fit_transform(train_docs_joined).toarray()\n",
    "    X_dev = vectorizer.transform(dev_docs_joined).toarray()\n",
    "    train_feats = [(X_train[i], train_labels[i]) for i in range(len(train_labels))]\n",
    "    dev_feats = [(X_dev[i], dev_labels[i]) for i in range(len(dev_labels))]\n",
    "    return train_feats, dev_feats\n",
    "\n",
    "\n",
    "train_feats_multinomial, dev_feats_multinomial = prepare_features(train_docs, dev_docs, train_labels, dev_labels, binary=False)\n",
    "train_feats_binarized, dev_feats_binarized = prepare_features(train_docs, dev_docs, train_labels, dev_labels, binary=True)\n",
    "sutils.create_training_graph(sutils.nn_helper, train_feats_multinomial, dev_feats_multinomial, kind=\"Neural Network Multinomial\", savepath=\"nn_training_graph_multinomial.png\")\n",
    "sutils.create_training_graph(sutils.nn_helper, train_feats_binarized, dev_feats_binarized, kind=\"Neural Network Binarized\", savepath=\"nn_training_graph_binary3.png\")\n",
    "\n",
    "precision_multinomial, recall_multinomial, f1_multinomial, accuracy_multinomial = sutils.nn_helper(\n",
    "    train_feats_multinomial, dev_feats_multinomial, epochs=10\n",
    ")\n",
    "precision_binarized, recall_binarized, f1_binarized, accuracy_binarized = sutils.nn_helper(\n",
    "    train_feats_binarized, dev_feats_binarized, epochs=10\n",
    ")\n",
    "\n",
    "print(\"F1 score with multinomial features:\", f1_multinomial)\n",
    "print(\"F1 score with binarized features:\", f1_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the f1 scores for your model with the following settings, using the same number of epochs to train in both cases:\n",
    "- number of epochs used: __10__\n",
    "- multinomial features: __.7378__ \n",
    "- binarized features: __.82__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
