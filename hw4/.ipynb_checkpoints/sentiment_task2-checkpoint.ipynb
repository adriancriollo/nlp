{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names \n",
    "----\n",
    "Names: __Adrian Criollo__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "\n",
    "Learn more about Naive Bayes here: https://www.nltk.org/_modules/nltk/classify/naivebayes.html \n",
    "\n",
    "Naive Bayes classifiers use Bayesâ€™ theorem for predictions. Naive Bayes can be a good baseline for NLP applications in particular. You can use it as a baseline for your project!\n",
    "\n",
    "**\n",
    "\n",
    "**10 points in Task 5 will be allocated for all 9 graphs (including the one generated here in Task 4 for Naive Bayes Classifier) being:**\n",
    "- Legible\n",
    "- Present below\n",
    "- Properly labeled\n",
    "     - x and y axes labeled\n",
    "     - Legend for accuracy measures plotted\n",
    "     - Plot Title with which model and run number the graph represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label: 0\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "def word_feats(words) -> dict:    \n",
    "    return dict([(word, True) for word in words])      \n",
    "\n",
    "\n",
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_FILE, header=None, sep='\\t', names = ['ID', 'Review', 'Label'])\n",
    "df_dev = pd.read_csv(DEV_FILE, header=None, sep='\\t', names = ['ID', 'Review', 'Label'])\n",
    "\n",
    "trained_data = []\n",
    "for review, label in zip(df_train['Review'], df_train['Label']):\n",
    "    words = review.split()\n",
    "    trained_data.append((word_feats(words), label))\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(trained_data)\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n",
    "\n",
    "test_review = df_dev['Review'].iloc[0].split()\n",
    "test_label = df_dev['Label'].iloc[0]\n",
    "prediction = classifier.classify(word_feats(first_dev_review))\n",
    "print(f\"Test label: {test_label}\")\n",
    "print(f\"Predicted label: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# NOTE : make sure one of your experiments uses 10% of the data, you will need this to answer the first question in task 5\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "def train_and_evaluate_classifier(train_data, dev_data, dev_labels, data_percentages):\n",
    "    \"\"\"\n",
    "    Trains a classifier on increasing portions of the training data and evaluates it on the dev set.\n",
    "    \n",
    "    Args:\n",
    "    - train_data: list of tuples (features, label) for training\n",
    "    - dev_data: list of dev examples for evaluation\n",
    "    - dev_labels: list of true dev labels\n",
    "    - data_percentages: list of percentages of training data to use for experiments\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of performance metrics (precision, recall, f1, accuracy) at each percentage\n",
    "    \"\"\"\n",
    "    metrics = {'precision': [], 'recall': [], 'f1': [], 'accuracy': []}\n",
    "    \n",
    "    for percentage in data_percentages:\n",
    "        # Calculate the amount of training data to use\n",
    "        data_size = int(len(train_data) * (percentage / 100))\n",
    "        current_train_data = train_data[:data_size]\n",
    "        \n",
    "        # Train the classifier\n",
    "        classifier = nltk.NaiveBayesClassifier.train(current_train_data)\n",
    "        \n",
    "        # Get predictions on the dev set\n",
    "        dev_preds = [classifier.classify(word_feats(review.split())) for review in dev_data]\n",
    "        \n",
    "        # Evaluate the classifier\n",
    "        precision_score = precision(refsets, testsets)\n",
    "        recall_score = recall(refsets, testsets)\n",
    "        f1_score = f_measure(refsets, testsets)\n",
    "        accuracy_score = accuracy(dev_labels, dev_preds)\n",
    "        \n",
    "        # Store the metrics for this percentage\n",
    "        metrics['precision'].append(precision_score)\n",
    "        metrics['recall'].append(recall_score)\n",
    "        metrics['f1'].append(f1_score)\n",
    "        metrics['accuracy'].append(accuracy_score)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example data percentages to experiment with\n",
    "data_percentages = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Get dev data and labels (assuming dev_data and dev_labels are already loaded)\n",
    "dev_data = df_dev['Review'].tolist()\n",
    "dev_labels = df_dev['Label'].tolist()\n",
    "\n",
    "# Assuming train_data is already prepared as (features, label) tuples\n",
    "metrics = train_and_evaluate_classifier(train_data, dev_data, dev_labels, data_percentages)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_percentages, metrics['precision'], label='Precision', marker='o')\n",
    "plt.plot(data_percentages, metrics['recall'], label='Recall', marker='o')\n",
    "plt.plot(data_percentages, metrics['f1'], label='F1 Score', marker='o')\n",
    "plt.plot(data_percentages, metrics['accuracy'], label='Accuracy', marker='o')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Classifier Performance on Dev Set as a Function of Training Data')\n",
    "plt.xlabel('Training Data Percentage (%)')\n",
    "plt.ylabel('Performance')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ (bag of words representation where we put 1 [true] if the word is there and 0 [false] otherwise) and a __multinomial__ (bag of words representation where we put the count of the word if the word occurs, and 0 otherwise). Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: __YOUR ANSWER HERE__\n",
    "- f1 score multinomial: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
