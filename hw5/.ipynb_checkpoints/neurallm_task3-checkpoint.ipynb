{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& 🎃 SpOoKy 👻 authors 🧟 data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\adria\\anaconda3\\lib\\site-packages (2.18.0rc2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0-rc2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0rc2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (1.67.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0-rc2->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0-rc2->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\adria\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\adria\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\adria\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0-rc2->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0-rc2->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0-rc2->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0-rc2->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0-rc2->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0-rc2->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0-rc2->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0-rc2->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\adria\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\adria\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0-rc2->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "!pip install tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import neurallm_utils as nutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "TRAIN_FILE = 'spooky_author_train.csv'\n",
    "spooky_author_data = pd.read_csv(TRAIN_FILE)\n",
    "spooky_text_data = spooky_author_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(spooky_text_data)\n",
    "word_encoded = word_tokenizer.texts_to_sequences(spooky_text_data)\n",
    "\n",
    "char_tokenizer = Tokenizer(char_level=True)\n",
    "char_tokenizer.fit_on_texts(spooky_text_data)\n",
    "char_encoded = char_tokenizer.texts_to_sequences(spooky_text_data)\n",
    "\n",
    "word_vocab_size = len(word_tokenizer.word_index)\n",
    "char_vocab_size = len(char_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25943\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "print(word_vocab_size)\n",
    "print(char_vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t                       y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[26, 2945, 143],\n",
       "  [2945, 143, 1372],\n",
       "  [143, 1372, 22],\n",
       "  [1372, 22, 36],\n",
       "  [22, 36, 294]],\n",
       " [[3, 9, 7], [9, 7, 8], [7, 8, 1], [8, 1, 20], [1, 20, 10]],\n",
       " 483974,\n",
       " 2879237)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    samples = []\n",
    "    for seq in encoded:\n",
    "        for i in range(ngram, len(seq) + 1):\n",
    "            samples.append(seq[i-ngram:i])\n",
    "    return samples\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "word_ngram_samples = generate_ngram_training_samples(word_encoded, NGRAM)\n",
    "char_ngram_samples = generate_ngram_training_samples(char_encoded, NGRAM)\n",
    "word_samples_preview = word_ngram_samples[:5]\n",
    "char_samples_preview = char_ngram_samples[:5]\n",
    "word_sequences_count = len(word_ngram_samples)\n",
    "char_sequences_count = len(char_ngram_samples)\n",
    "\n",
    "word_samples_preview, char_samples_preview, word_sequences_count, char_sequences_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level shapes:\n",
      "X_word shape: (483974, 2), y_word shape: (483974,)\n",
      "\n",
      "Character-level shapes:\n",
      "X_char shape: (2879237, 2), y_char shape: (2879237,)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "\n",
    "\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "\n",
    "def split_sequences(sequences):\n",
    "    X = [seq[:-1] for seq in sequences]\n",
    "    y = [seq[-1] for seq in sequences]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_word, y_word = split_sequences(word_ngram_samples)\n",
    "X_char, y_char = split_sequences(char_ngram_samples)\n",
    "\n",
    "print(\"Word-level shapes:\")\n",
    "print(f\"X_word shape: {X_word.shape}, y_word shape: {y_word.shape}\")\n",
    "\n",
    "print(\"\\nCharacter-level shapes:\")\n",
    "print(f\"X_char shape: {X_char.shape}, y_char shape: {y_char.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    word_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    index_to_embedding[0] = np.zeros(EMBEDDINGS_SIZE)\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_embedding[word] = vector\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if word in word_to_embedding:\n",
    "            index_to_embedding[index] = word_to_embedding[word]\n",
    "        else:\n",
    "            index_to_embedding[index] = np.random.uniform(-0.01, 0.01, EMBEDDINGS_SIZE)\n",
    "\n",
    "    return word_to_embedding, index_to_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict, num_classes: int):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    Returns data generator to be used by feed_forward\n",
    "    '''\n",
    "    num_samples = len(X)\n",
    "    embedding_dim = next(iter(index_2_embedding.values())).shape[0]\n",
    "    sequence_length = len(X[0])\n",
    "\n",
    "    while True:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = [X[i] for i in indices]\n",
    "        y_shuffled = [y[i] for i in indices]\n",
    "\n",
    "        for offset in range(0, num_samples, num_sequences_per_batch):\n",
    "            end = offset + num_sequences_per_batch\n",
    "            X_batch_indices = X_shuffled[offset:end]\n",
    "            y_batch_indices = y_shuffled[offset:end]\n",
    "\n",
    "            current_batch_size = len(X_batch_indices)\n",
    "\n",
    "            X_batch = np.zeros((current_batch_size, sequence_length, embedding_dim), dtype='float32')\n",
    "            y_batch = np.zeros((current_batch_size, num_classes), dtype='float32')\n",
    "\n",
    "            for i, seq in enumerate(X_batch_indices):\n",
    "                for j, index in enumerate(seq):\n",
    "                    X_batch[i, j] = index_2_embedding.get(index, np.zeros(embedding_dim))\n",
    "                y_batch[i] = to_categorical(y_batch_indices[i], num_classes=num_classes)\n",
    "            yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word sample shapes:\n",
      "Input shape: (128, 2, 50)\n",
      "Label shape: (128, 25944)\n",
      "\n",
      "Character sample shapes:\n",
      "Input shape: (128, 2, 50)\n",
      "Label shape: (128, 59)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "# Examples:\n",
    "# num_sequences_per_batch = 128 # this is the batch size\n",
    "# steps_per_epoch = len(sequences)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "num_sequences_per_batch = 128\n",
    "word_embeddings_file = 'spooky_embedding_word.txt'\n",
    "_, index_to_embedding_word = read_embeddings(word_embeddings_file, word_tokenizer)\n",
    "char_embeddings_file = 'spooky_embedding_char.txt'\n",
    "_, index_to_embedding_char = read_embeddings(char_embeddings_file, char_tokenizer)\n",
    "vocab_size_word = len(word_tokenizer.word_index)\n",
    "num_classes_word = vocab_size_word\n",
    "vocab_size_char = len(char_tokenizer.word_index)\n",
    "num_classes_char = vocab_size_char\n",
    "word_train_generator = data_generator(\n",
    "    X_word, y_word, num_sequences_per_batch, index_to_embedding_word, num_classes_word\n",
    ")\n",
    "\n",
    "char_train_generator = data_generator(\n",
    "    X_char, y_char, num_sequences_per_batch, index_to_embedding_char, num_classes_char\n",
    ")\n",
    "\n",
    "word_sample = next(word_train_generator)\n",
    "print(\"Word sample shapes:\")\n",
    "print(\"Input shape:\", word_sample[0].shape)\n",
    "print(\"Label shape:\", word_sample[1].shape)\n",
    "char_sample = next(char_train_generator)\n",
    "print(\"\\nCharacter sample shapes:\")\n",
    "print(\"Input shape:\", char_sample[0].shape)\n",
    "print(\"Label shape:\", char_sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on our machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired\n",
    "\n",
    "def build_feedforward_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "steps_per_epoch_word = len(X_word) // num_sequences_per_batch\n",
    "sequence_length_word = X_word.shape[1]\n",
    "embedding_dim_word = EMBEDDINGS_SIZE\n",
    "input_shape_word = (sequence_length_word, embedding_dim_word)\n",
    "vocab_size_word = len(word_tokenizer.word_index) + 1 \n",
    "num_classes_word = vocab_size_word\n",
    "word_train_generator = data_generator(X_word, y_word, num_sequences_per_batch, index_to_embedding_word, num_classes_word)\n",
    "model_word = build_feedforward_model(input_shape_word, num_classes_word)\n",
    "\n",
    "\n",
    "\n",
    "steps_per_epoch_char = len(X_char) // num_sequences_per_batch\n",
    "sequence_length_char = X_char.shape[1]\n",
    "embedding_dim_char = EMBEDDINGS_SIZE\n",
    "input_shape_char = (sequence_length_char, embedding_dim_char)\n",
    "vocab_size_char = len(char_tokenizer.word_index) + 1\n",
    "num_classes_char = vocab_size_char\n",
    "char_train_generator = data_generator(X_char, y_char, num_sequences_per_batch, index_to_embedding_char, num_classes_char)\n",
    "model_char = build_feedforward_model(input_shape_char, num_classes_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3781/3781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 57ms/step - accuracy: 0.1058 - loss: 6.9168\n",
      "Epoch 2/5\n",
      "\u001b[1m3781/3781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 49ms/step - accuracy: 0.1342 - loss: 6.0239\n",
      "Epoch 3/5\n",
      "\u001b[1m3781/3781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 42ms/step - accuracy: 0.1381 - loss: 5.7224\n",
      "Epoch 4/5\n",
      "\u001b[1m3781/3781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 42ms/step - accuracy: 0.1408 - loss: 5.4905\n",
      "Epoch 5/5\n",
      "\u001b[1m3781/3781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 42ms/step - accuracy: 0.1426 - loss: 5.3033\n",
      "Epoch 1/5\n",
      "\u001b[1m22494/22494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4ms/step - accuracy: 0.3439 - loss: 2.2361\n",
      "Epoch 2/5\n",
      "\u001b[1m22494/22494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4ms/step - accuracy: 0.3787 - loss: 2.0218\n",
      "Epoch 3/5\n",
      "\u001b[1m22494/22494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4ms/step - accuracy: 0.3812 - loss: 2.0027\n",
      "Epoch 4/5\n",
      "\u001b[1m22494/22494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4ms/step - accuracy: 0.3812 - loss: 1.9951\n",
      "Epoch 5/5\n",
      "\u001b[1m22494/22494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4ms/step - accuracy: 0.3822 - loss: 1.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2544bb422a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on our computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on our computer\n",
    "# results in accuracy of 0.2110\n",
    "\n",
    "model_word.fit(\n",
    "    x=word_train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_word,\n",
    "    epochs=5\n",
    ")\n",
    "model_char.fit(\n",
    "    x=char_train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_char,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "\n",
    "\n",
    "\n",
    "model_word.save(\"word_language_model.h5\")\n",
    "model_char.save(\"char_language_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# load your models if you need to\n",
    "loaded_word_model = load_model(\"word_language_model.h5\")\n",
    "loaded_char_model = load_model(\"char_language_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model, tokenizer, seed, index_to_embedding, max_length=50, end_token='</s>'):\n",
    "    result_sequence = seed.copy()\n",
    "    context_window = seed.copy()\n",
    "\n",
    "    embedding_dim = next(iter(index_to_embedding.values())).shape[0]\n",
    "    sequence_length = len(context_window)\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    index_to_word[0] = '<PAD>'\n",
    "\n",
    "    eos_index = tokenizer.word_index.get(end_token)\n",
    "    if eos_index is None:\n",
    "        eos_index = None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        input_embeddings = np.array([\n",
    "            index_to_embedding.get(idx, np.zeros(embedding_dim)) for idx in context_window\n",
    "        ])\n",
    "        input_embeddings = input_embeddings.reshape(1, sequence_length, embedding_dim)\n",
    "\n",
    "        yhat = model.predict(input_embeddings, verbose=0)\n",
    "        next_index = np.argmax(yhat, axis=-1)[0]\n",
    "\n",
    "        result_sequence.append(next_index)\n",
    "\n",
    "        if eos_index is not None and next_index == eos_index:\n",
    "            break\n",
    "\n",
    "        context_window.append(next_index)\n",
    "        context_window = context_window[1:]\n",
    "\n",
    "    generated_words = [index_to_word.get(idx, '<UNK>') for idx in result_sequence]\n",
    "    generated_words = [word for word in generated_words if word not in ('<s>', '</s>')]\n",
    "    generated_sentence = ' '.join(generated_words).replace('_', ' ')\n",
    "\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence from the word model:\n",
      "this process was not to be sure to be sure to be sure to be sure to be sure to be sure\n",
      "\n",
      "Generated sentence from the character model:\n",
      "t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e   t h e  \n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer\n",
    "\n",
    "seed_sequence_word = X_word[0].tolist()\n",
    "\n",
    "generated_sentence_word = generate_seq(\n",
    "    model=model_word,\n",
    "    tokenizer=word_tokenizer,\n",
    "    seed=seed_sequence_word,\n",
    "    index_to_embedding=index_to_embedding_word,\n",
    "    max_length=20,\n",
    "    end_token='</s>' \n",
    ")\n",
    "\n",
    "print(\"Generated sentence from the word model:\")\n",
    "print(generated_sentence_word)\n",
    "\n",
    "\n",
    "seed_sequence_char = X_char[0].tolist()\n",
    "\n",
    "generated_sentence_char = generate_seq(\n",
    "    model=model_char,\n",
    "    tokenizer=char_tokenizer,\n",
    "    seed=seed_sequence_char,\n",
    "    index_to_embedding=index_to_embedding_char,\n",
    "    max_length=50,\n",
    "    end_token='</s>'\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated sentence from the character model:\")\n",
    "print(generated_sentence_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "def generate_sentences(model, tokenizer, X_data, index_to_embedding, num_sentences, max_length=20, end_token='</s>'):\n",
    "    sentences = []\n",
    "\n",
    "    for _ in range(num_sentences):\n",
    "        seed_sequence = random.choice(X_data).tolist()\n",
    "        generated_sequence = generate_seq(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            seed=seed_sequence,\n",
    "            index_to_embedding=index_to_embedding,\n",
    "            max_length=max_length,\n",
    "            end_token=end_token\n",
    "        )\n",
    "\n",
    "        sentences.append(generated_sequence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "num_sentences = 100\n",
    "max_length_word = 20\n",
    "generated_sentences_word = generate_sentences(\n",
    "    model=model_word,\n",
    "    tokenizer=word_tokenizer,\n",
    "    X_data=X_word,\n",
    "    index_to_embedding=index_to_embedding_word,\n",
    "    num_sentences=num_sentences,\n",
    "    max_length=max_length_word,\n",
    "    end_token='</s>'\n",
    ")\n",
    "\n",
    "output_file_word = 'generated_sentences_word.txt'\n",
    "with open(output_file_word, 'w', encoding='utf-8') as f:\n",
    "    for sentence in generated_sentences_word:\n",
    "        sentence_clean = ' '.join([word for word in sentence.split() if word not in ('<s>', '</s>')])\n",
    "        f.write(sentence_clean + '\\n')\n",
    "\n",
    "max_length_char = 50\n",
    "generated_sentences_char = generate_sentences(\n",
    "    model=model_char,\n",
    "    tokenizer=char_tokenizer,\n",
    "    X_data=X_char,\n",
    "    index_to_embedding=index_to_embedding_char,\n",
    "    num_sentences=num_sentences,\n",
    "    max_length=max_length_char,\n",
    "    end_token='</s>'\n",
    ")\n",
    "\n",
    "output_file_char = 'generated_sentences_char.txt'\n",
    "with open(output_file_char, 'w', encoding='utf-8') as f:\n",
    "    for sentence in generated_sentences_char:\n",
    "        sentence_clean = ' '.join([char for char in sentence.split() if char not in ('<s>', '</s>')])\n",
    "        f.write(sentence_clean + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
